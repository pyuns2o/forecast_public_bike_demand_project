{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "path = '/content/drive/MyDrive/통계적기계학습_팀플/데이터/대여이력정보/1~6'\n",
    "\n",
    "forder = os.listdir(path)\n",
    "\n",
    "data1 = pd.DataFrame()\n",
    "\n",
    "for files in tqdm(forder):\n",
    "    data_ = pd.read_csv(path+'/'+files, encoding='cp949')\n",
    "    data1 = pd.concat([data1, data_])\n",
    "\n",
    "path = '/content/drive/MyDrive/통계적기계학습_팀플/데이터/대여이력정보/7~12'\n",
    "\n",
    "forder = os.listdir(path)\n",
    "\n",
    "data2 = pd.DataFrame()\n",
    "\n",
    "for files in tqdm(forder):\n",
    "    data_ = pd.read_csv(path+'/'+files, encoding='cp949')\n",
    "    data2 = pd.concat([data2, data_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = ['대여일시', '대여 대여소번호', '반납일시', '반납대여소번호', '이용시간', '이용거리']\n",
    "\n",
    "# 강서구 대여소 번호\n",
    "gangseogu_num = pd.read_excel('/content/drive/MyDrive/통계적기계학습_팀플/데이터/강서구대여소번호.xlsx', engine='openpyxl', header=None)\n",
    "\n",
    "gangseogu_num = list(gangseogu_num[0].values)\n",
    "\n",
    "data1 = data1[use]\n",
    "data1 = data1[data1['대여 대여소번호'].isin(gangseogu_num)]\n",
    "data2 = data2[['대여일시', '대여 대여소번호', '반납일시', '반납대여소번호', '이용시간(분)', '이용거리(M)']]\n",
    "data2.columns = use\n",
    "data2 = data2[data2['대여 대여소번호'].isin(gangseogu_num)]\n",
    "\n",
    "data = pd.concat([data1, data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['대여일시'] = data['대여일시'].apply(lambda x: x[:13])\n",
    "data['반납일시'] = data['반납일시'].apply(lambda x: x[:13])\n",
    "\n",
    "error_cnt = 0\n",
    "\n",
    "def convert(x):\n",
    "    global error_cnt\n",
    "    try:\n",
    "        x = str(int(x))\n",
    "        return x\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "data['반납대여소번호'] = data['반납대여소번호'].apply(convert)\n",
    "data['대여 대여소번호'] = data['대여 대여소번호'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/content/drive/MyDrive/통계적기계학습_팀플/데이터/강서구_대여이력정보.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas\n",
    "\n",
    "def date_range(start, end):\n",
    "    start = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    dates = []\n",
    "    for date in pandas.date_range(start, periods=(end-start).days+1):\n",
    "        date = date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        for i in range(24):\n",
    "            i = str(i)\n",
    "            if int(i) < 10: i = '0' + i\n",
    "            dates.append(date + ' ' + i)\n",
    "\n",
    "    return dates\n",
    "\n",
    "dates = date_range(\"2022-01-01\", \"2022-12-31\")\n",
    "\n",
    "rental_grouped = data.groupby(data['대여 대여소번호'])\n",
    "return_grouped = data.groupby(data['반납대여소번호'])\n",
    "\n",
    "final_data_dict = {}\n",
    "\n",
    "    for num in tqdm(gangseogu_num):\n",
    "    try:\n",
    "        rental_data = rental_grouped.get_group(str(num))\n",
    "        return_data = return_grouped.get_group(str(num))\n",
    "        rental_data_ = rental_data.groupby('대여일시')\n",
    "\n",
    "        rental_data_dict = {}\n",
    "\n",
    "        for d in rental_data_:\n",
    "            time_df = d[1] # 특정 시간에서 데이터\n",
    "\n",
    "            col_0 = time_df.iloc[0]['대여일시']\n",
    "            col_1 = len(time_df)\n",
    "            col_2 = time_df['이용거리'].sum()\n",
    "            col_3 = time_df['이용시간'].sum()\n",
    "\n",
    "            rental_data_dict[col_0] = [col_0, col_1, col_2, col_3]\n",
    "\n",
    "        return_data_ = return_data.groupby('반납일시')\n",
    "\n",
    "        return_data_dict = {}\n",
    "\n",
    "        for d in return_data_:\n",
    "            time_df = d[1]\n",
    "\n",
    "            col_0 = time_df.iloc[0]['반납일시']\n",
    "            col_1 = len(time_df)\n",
    "            col_2 = time_df['이용거리'].sum()\n",
    "            col_3 = time_df['이용시간'].sum()\n",
    "            \n",
    "            return_data_dict[col_0] = [col_0, col_1, col_2, col_3]\n",
    "\n",
    "        new_data = []\n",
    "\n",
    "        rental_data_keys = rental_data_dict.keys()\n",
    "        return_data_keys = return_data_dict.keys()\n",
    "\n",
    "        for t in dates:\n",
    "            if t in rental_data_keys and t in return_data_keys:\n",
    "                new_data.append(rental_data_dict[t] + return_data_dict[t][1:])\n",
    "            elif t in rental_data_keys and t not in return_data_keys:\n",
    "                new_data.append(rental_data_dict[t] + [0, 0, 0])\n",
    "            elif t not in rental_data_keys and t in return_data_keys:\n",
    "                new_data.append([t] + [0, 0, 0] + return_data_dict[t][1:])\n",
    "            elif t not in rental_data_keys and t not in return_data_keys:\n",
    "                new_data.append([t] + [0, 0, 0, 0, 0, 0])\n",
    "\n",
    "        new_data = pd.DataFrame(\n",
    "            new_data,\n",
    "            columns=['시간대', '대여', '대여_이용거리', '대여_이용시간', '반납', '반납_이용거리', '반납_이용시간']\n",
    "            )\n",
    "\n",
    "        new_data['반납-대여'] = new_data['반납'] - new_data['대여']\n",
    "\n",
    "        final_data_dict[str(num)] = new_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날씨(기온, 습도), 미세먼지(미세먼지, 초미세먼지) 데이터\n",
    "weather = pd.read_csv(\"/content/drive/MyDrive/통계적기계학습_팀플/데이터/weather_2022_01_01to2022_12_31.csv\")\n",
    "\n",
    "for i in weather.index:\n",
    "    w = weather.loc[i, 'Time']\n",
    "    if w[-2:] != '00':\n",
    "        weather.drop(i, axis=0, inplace=True)\n",
    "\n",
    "weather = weather.reset_index(drop=True)\n",
    "weather = weather[['Time', 'Temperature', 'Rel. humidity']]\n",
    "weather['Temperature'] = weather['Temperature'].apply(lambda x: int(x[:-2]))\n",
    "weather['Rel. humidity'] = weather['Rel. humidity'].apply(lambda x: int(x[:-1]))\n",
    "weather['Time'] = weather['Time'].apply(lambda x: x[:2])\n",
    "\n",
    "time = []\n",
    "\n",
    "for t in range(24):\n",
    "    if t < 10:\n",
    "        time.append('0'+str(t))\n",
    "    else:\n",
    "        time.append(str(t))\n",
    "time *= 365\n",
    "i = 0\n",
    "\n",
    "while i < 24*365:\n",
    "    w = weather.loc[i, 'Time']\n",
    "    if w != time[i]:\n",
    "        temp = (weather.loc[i, 'Temperature'] + weather.loc[i+1, 'Temperature']) / 2\n",
    "        humid = (weather.loc[i, 'Rel. humidity'] + weather.loc[i+1, 'Rel. humidity']) / 2\n",
    "        weather.loc[i+0.5] = [time[i], temp, humid]\n",
    "    weather = weather.sort_index()\n",
    "    weather = weather.reset_index(drop=True)\n",
    "    i += 1\n",
    "    i += 1\n",
    "\n",
    "weather = weather.drop('Time', axis=1)\n",
    "\n",
    "dust = pd.read_csv(\"/content/drive/MyDrive/통계적기계학습_팀플/데이터/강서구 대기질 자료 제공_2022.csv\")\n",
    "dust = dust[['미세먼지(PM10)', '초미세먼지(PM2.5)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/content/drive/MyDrive/통계적기계학습_팀플/데이터/강서구_시간대여소별_대여이력정보.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, scaler, window_size):\n",
    "        self.scaler = scaler\n",
    "        if self.scaler:\n",
    "            data = self.scaler.transform(data)\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            data = self.scaler.fit_transform(data)\n",
    "        \n",
    "        x, y = [], []\n",
    "        for i in range(len(data) - window_size):\n",
    "            x.append(data[i:i+window_size])\n",
    "            y.append(data[i+window_size, 0])\n",
    "        \n",
    "        self.x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        x, _ = self.lstm(x, (h_0, c_0))\n",
    "        x = self.linear(x[:, -1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.loss = nn.MSELoss().to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_history = {'train': [], 'val': []}\n",
    "    \n",
    "    def train(self):\n",
    "        min_val_loss = None\n",
    "        cnt = 0\n",
    "        for epoch in range(total_epochs):\n",
    "            print(f'Epoch: {epoch:02}')\n",
    "\n",
    "            train_loss = self.train_step()\n",
    "            print(f'\\tTrain Loss: {train_loss:.5f}')\n",
    "\n",
    "            val_loss = self.val_step()\n",
    "            print(f'\\tVal Loss: {val_loss:.5f}')\n",
    "\n",
    "            self.loss_history['train'].append(train_loss)\n",
    "            self.loss_history['val'].append(val_loss)\n",
    "            \n",
    "            if min_val_loss is None or val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                cnt = 0\n",
    "            else:\n",
    "                cnt += 1\n",
    "            print(f'patience: {cnt}')\n",
    "            if cnt >= 10:\n",
    "                print('Early stopped!')\n",
    "                break\n",
    "    \n",
    "    def train_step(self):\n",
    "        self.model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        bar = tqdm(self.train_loader)\n",
    "\n",
    "        for batch in bar:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            x, y = batch\n",
    "            outputs = self.model(x)\n",
    "\n",
    "            loss = self.loss(outputs.squeeze(), y)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            bar.set_description(f'Train Loss: {loss.item():.5f}')\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= len(self.train_loader)\n",
    "        return epoch_loss\n",
    "    \n",
    "    def val_step(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        bar = tqdm(self.val_loader)\n",
    "\n",
    "        for batch in bar:\n",
    "            x, y = batch\n",
    "            outputs = self.model(x)\n",
    "            loss = self.loss(outputs.squeeze(), y)\n",
    "            bar.set_description(f'Val Loss: {loss.item():.5f}')\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        epoch_loss /= len(self.val_loader)\n",
    "        return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = ['대여', '대여_이용거리', '대여_이용시간', '반납', '반납_이용거리', '반납_이용시간', '반납-대여']\n",
    "\n",
    "window_size = 4 # 연속된 window_size개의 데이터를 가지고 다음 값 예측\n",
    "input_size = len(use_features) + 4 # input 데이터 차원\n",
    "hidden_size = 36 # lstm hidden state 차원\n",
    "num_layers = 2 # lstm 쌓은 층\n",
    "output_size = 1 # output 차원\n",
    "dropout = 0.2\n",
    "lr = 0.003\n",
    "total_epochs = 500\n",
    "batch_size = 10000\n",
    "train_val_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset with each place and concat it\n",
    "testdata_dict = {}\n",
    "dataset_dict = {}\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for k, d in tqdm(data.items()):\n",
    "    d = pd.concat([d[use_features], weather, dust], axis=1)\n",
    "    testdata_dict[k] = d.iloc[-24*7:, :].reset_index(drop=True)  # last week is test data\n",
    "\n",
    "scaler.fit(d.iloc[:-24*7, :])\n",
    "\n",
    "for k, d in tqdm(data.items()):\n",
    "    d = pd.concat([d[use_features], weather, dust], axis=1)\n",
    "    dataset_dict[k] = TimeSeriesDataset(d.iloc[:-24*7, :], scaler=scaler, window_size=window_size)\n",
    "\n",
    "dataset = ConcatDataset(list(dataset_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_val_ratio, 1-train_val_ratio])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(model=model, train_loader=train_loader, val_loader=val_loader)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(trainer.loss_history['train'])), trainer.loss_history['train'], label='train loss')\n",
    "plt.plot(range(len(trainer.loss_history['train'])), trainer.loss_history['val'], label='val loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset_dict = {}\n",
    "for k, v in testdata_dict.items():\n",
    "    testdataset_dict[k] = TimeSeriesDataset(v, scaler=scaler, window_size=window_size)\n",
    "\n",
    "def predict(model, rental_place):\n",
    "    model.eval()\n",
    "    pred = model(testdataset_dict[rental_place].x)\n",
    "    pred_with_dummy = torch.cat([torch.zeros((164, 6)), pred.to('cpu'), torch.zeros((164, 4))], axis=1).detach().numpy()\n",
    "    pred = testdataset_dict[rental_place].scaler.inverse_transform(pred_with_dummy)[:, 6]\n",
    "    return pred\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/통계적기계학습_팀플/lstm_all_features_minmax.pt'))\n",
    "model.to(device)\n",
    "\n",
    "target_place = '2701'\n",
    "pred = predict(model, target_place)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f'Place: {target_place}\\nTime: 2022-12-24 04:00 ~ 2022-12-31 23:00')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('return - rental')\n",
    "plt.plot(pred, label='pred')\n",
    "plt.plot(testdata_dict[target_place].loc[4:, '반납-대여'].values, label='true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = predict(model, '1178')\n",
    "pred2 = predict(model, '3754')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Two places\\nTime: 2022-12-24 04:00 ~ 2022-12-31 23:00')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('return - rental')\n",
    "plt.plot(pred1, label='1178')\n",
    "plt.plot(pred2, label='3754')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
